{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Distribution for THINGS\n",
        "\n",
        "A big part of why things will work or not work in this project is down to the similarity of functions with each other. I mean both how diverse the training distribution is, but also how similar the test functions are to the training functions. Here, I try to understand some of this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "from metarep.data import prepare_things_spose\n",
        "\n",
        "num_positive = 50\n",
        "np.random.seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quantifying Problem Similarity\n",
        "\n",
        "Simply train a linear classifier from the dino embeddings to every single SPoSE dimension seperately, and save the weight vectors for each problem. There's probably a better way to do this, but for now, it's good.\n",
        "\n",
        "We assign the top `num_positive` instances from a given dimension to the positive label, and we get the bottom `num_positive` entries for the negative class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pipeline  = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    LogisticRegression(max_iter=1000, random_state=1234, n_jobs=1)\n",
        ")\n",
        "X, Y = prepare_things_spose(np.load(\"data/backbone_reps/dinov2_vitb14_reg.npz\"), return_tensors=\"np\")\n",
        "# do the same as above but with spose targets\n",
        "weights = []\n",
        "for og_id in tqdm(range(Y.shape[1])):\n",
        "    # get row ids of the top num_positive activations for this column\n",
        "    top_ids = np.argsort(Y[:, og_id])[-num_positive:]\n",
        "    # get row ids of the bottom num_positive activations\n",
        "    bottom_ids = np.argsort(Y[:, og_id])[:num_positive]\n",
        "\n",
        "    X_sub = X[np.concatenate([top_ids, bottom_ids])]\n",
        "    y_sub = np.concatenate([np.ones(num_positive), np.zeros(num_positive)])\n",
        "\n",
        "    # fit the model\n",
        "    pipeline.fit(X_sub, y_sub)\n",
        "    # get the weights of the model\n",
        "    weights.append(pipeline.named_steps['logisticregression'].coef_[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, compute the pairwise similarity of the weight vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "weights = np.array(weights)\n",
        "cosine_weights = 1 - pairwise_distances(weights, metric='cosine')\n",
        "cosine_weights_flat = cosine_weights.flatten()\n",
        "# remove self similarity\n",
        "cosine_weights_flat = cosine_weights_flat[cosine_weights_flat != 1.0]\n",
        "cosine_weights_test = cosine_weights[:3, :3]\n",
        "cosine_weights_test_flat = cosine_weights_test.flatten()\n",
        "cosine_weights_test_flat = cosine_weights_test_flat[cosine_weights_test_flat != 1.0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cmap = sns.color_palette()\n",
        "\n",
        "# Set up a GridSpec layout: 2 rows, 2 columns\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "gs = fig.add_gridspec(nrows=2, ncols=2, height_ratios=[1, 1])\n",
        "\n",
        "# First row: two heatmaps\n",
        "ax0 = fig.add_subplot(gs[0, 0])\n",
        "sns.heatmap(cosine_weights, ax=ax0, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "ax0.set_title(\"Similarity of All Functions\")\n",
        "ax0.collections[0].colorbar.remove()  # Remove colorbar from first heatmap\n",
        "\n",
        "ax1 = fig.add_subplot(gs[0, 1])\n",
        "sns.heatmap(cosine_weights_test, ax=ax1, cmap='coolwarm', vmin=-1, vmax=1, annot=True, fmt=\".2f\")\n",
        "ax1.set_title(\"Similarity of Test Functions\")\n",
        "\n",
        "# Second row: full-width KDE plot\n",
        "ax2 = fig.add_subplot(gs[1, :])\n",
        "sns.kdeplot(cosine_weights_flat, ax=ax2, label='Full Similarity Distribution', fill=True, alpha=0.5, color=cmap[0])\n",
        "for i in range(3):\n",
        "    ax2.axvline(cosine_weights[i, :].mean(), color=cmap[-i], linestyle='--', alpha=1,\n",
        "                label=f'Feature {i} Average  Similarity')\n",
        "    \n",
        "\n",
        "# also plot the average cosine similarity of the whole matrix\n",
        "ax2.axvline(cosine_weights.mean(), color='black', linestyle='--', alpha=1., label='Average Pairwise Similarity')\n",
        "    \n",
        "# x min and max at -0.5 and .5\n",
        "ax2.set_xlim(-0.5, 0.5)\n",
        "ax2.set_title(\"Distribution of Similarities\")\n",
        "ax2.set_xlabel(\"Similarity\")\n",
        "ax2.set_ylabel(\"Density\")\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The test dimensions do seem to contain three \"distinct\" functions. They are also not too similar to any of the training functions, though this is hard to quantify.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "similarities_to_average = []\n",
        "average_weight = weights[3:].mean(0).reshape(1, -1)\n",
        "for i in range(weights.shape[0]):\n",
        "    similarities_to_average.append(1 - pairwise_distances(average_weight, weights[i].reshape(1, -1), metric='cosine'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What about the similarity of the test functions to the averaged classifier weight vector, over the entire training functions? Again, it seems like the test functions are not \"too\" similar to the average of the training functions. Though, dimension 0 seems more similar than the other two. \n",
        "\n",
        "This actually shows if you provide uninformative labels in-context, where the model gets higher than chance level accuracy, only on dimension 0. I believe, it's because it's prior after training is pointing towards this function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax.set_title(\"Similarity of Average Weight with Each Function\")\n",
        "ax.set_xlabel(\"Function Index\")\n",
        "ax.set_ylabel(\"Similarity\")\n",
        "ax.scatter(range(len(similarities_to_average)), similarities_to_average)\n",
        "# the first three points should have a different color\n",
        "ax.scatter(range(3), similarities_to_average[:3], color='black', label='Test Functions')\n",
        "ax.axhline(np.mean(similarities_to_average), color='black', linestyle='--', label='Mean Similarity')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/candemircan/Projects/cpi/metarep/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}