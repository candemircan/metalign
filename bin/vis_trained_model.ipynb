{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does a trained model learn?\n",
    "\n",
    "Once I train a model on a distribution of classification tasks, I'm interested in\n",
    "\n",
    "- Do the representations of the model become more aligned with those of humans?\n",
    "- Can the model show human-like learning behaviour.\n",
    "\n",
    "At the moment, the model training part is quite shaky. Nevertheless, there are some interesting preliminary stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nnsight import NNsight\n",
    "\n",
    "from metalign.data import ThingsFunctionLearning, prepare_things_spose\n",
    "from metalign.model import Transformer, TransformerConfig\n",
    "\n",
    "_ = torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(TransformerConfig(intermediate_size=3072,input_size=2306, num_attention_heads=12))\n",
    "model.load_state_dict(torch.load(\"data/checkpoints/cluster_full_attempt2/best.pt\",map_location=torch.device('cpu'))[\"model_state_dict\"], strict=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representational Alignment\n",
    "\n",
    "Here, I want to compare the meta-learned model's representations to the SPoSE. This I will do through CKA.\n",
    "\n",
    "The meta-learned model is a sequence model. For this comparison, I'll keep the sequence length at 1.\n",
    "\n",
    "The model also expects the ground truth label from the previous item in the sequence as a one-hot vector prepended to the input. In this case (and in any cases for the first element in the sequence), we set it to `[0 0]`.\n",
    "\n",
    "I'll not only compare the representations of this model over layers to the SPoSE, but also compare the raw Dino-v2 embeddings to SPoSE, which will serve as a baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_og, Y = prepare_things_spose(np.load(\"data/backbone_reps/dinov2_vitb14_reg.npz\"))\n",
    "X = torch.cat([torch.zeros(X_og.shape[0], 2), X_og], dim=1)  # prepend 0 0 to each row\n",
    "# let's pretend we have a sequence of 1\n",
    "X = X.unsqueeze(1)  # add sequence dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need the CKA function here, which computes the cosine similarity between two centered and flattened linear kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "def cka(\n",
    "    X: torch.Tensor,  # Representations of the first set of samples\n",
    "    Y: torch.Tensor,  # Representations of the second set of samples\n",
    ") -> torch.Tensor:  # The linear CKA between X and Y\n",
    "    \"Compute the linear CKA between two matrices X and Y.\"\n",
    "    X -= X.mean(dim=0)\n",
    "    Y -= Y.mean(dim=0)\n",
    "\n",
    "    XTX = X.T @ X\n",
    "    YTY = Y.T @ Y\n",
    "    YTX = Y.T @ X\n",
    "\n",
    "    return (YTX**2).sum() / torch.sqrt((XTX**2).sum() * (YTY**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnsight_model = NNsight(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_cka = cka(X_og, Y)\n",
    "baseline_cka_test = cka(X_og, Y[:,:3])\n",
    "\n",
    "layers = []\n",
    "with nnsight_model.trace(X):\n",
    "   layers.append(nnsight_model.embedding.output.squeeze().save())\n",
    "   for layer in nnsight_model.layers:\n",
    "       layers.append(layer.output.squeeze().save())\n",
    "\n",
    "\n",
    "cka_results = []\n",
    "cka_test_results = []\n",
    "for i in range(len(layers)):\n",
    "    cka_number = cka(layers[i], Y)\n",
    "    cka_results.append(cka_number.item())\n",
    "    cka_test_number = cka(layers[i], Y[:,:3])\n",
    "    cka_test_results.append(cka_test_number.item())       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(12, 6))\n",
    "axs[0].plot(cka_results, marker='o', label=\"Meta-Learned\")\n",
    "axs[0].axhline(baseline_cka, color=plt.rcParams['axes.prop_cycle'].by_key()['color'][-1], linestyle='--', label=\"Baseline\")\n",
    "axs[0].set_xlabel(\"Layer\")\n",
    "# remove legend\n",
    "axs[0].legend().remove()\n",
    "axs[0].set_ylabel(\"CKA with Hebart Features\")\n",
    "axs[0].set_title(\"All Dimensions\")\n",
    "axs[0].set_xticks(range(len(cka_results)))\n",
    "axs[0].set_xticklabels(range(len(cka_results)))\n",
    "fig.legend(loc='upper center', ncol=2, frameon=False, bbox_to_anchor=(0.52, 1))\n",
    "\n",
    "axs[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.2f}'))\n",
    "\n",
    "axs[1].plot(cka_test_results, marker='o', label=\"Meta-Learned\")\n",
    "axs[1].axhline(baseline_cka_test, color=plt.rcParams['axes.prop_cycle'].by_key()['color'][-1], linestyle='--', label=\"Baseline\")\n",
    "# no frame on legend\n",
    "axs[1].set_xlabel(\"Layer\")\n",
    "axs[1].set_ylabel(\"\")\n",
    "axs[1].set_title(\"Only Eval Dimensions\")\n",
    "# x ticks should be integers just\n",
    "axs[1].set_xticks(range(len(cka_test_results)))\n",
    "axs[1].set_xticklabels(range(len(cka_test_results)))\n",
    "axs[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.2f}'))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across all layers, the representations of the meta-learned model are more similar to SPoSE than how similar the original Dino v2 representations are to SPoSE. \n",
    "\n",
    "This is not surprising, given that the model is trained on these dimensions. But, it's a good sanity check.\n",
    "\n",
    "What's maybe a bit more impressive is, if I do CKA with the eval dimensions (0, 1, and 2 in SPoSE), we still have the same pattern. These dimensions come from the same embedding, and were learned jointly with the training dimensions. However, they were not directly exposed during training. Nice!\n",
    "\n",
    "## Behavioural Alignment\n",
    "\n",
    "Next, I look at the learning curves of the meta-learner, and compare it to learning curves of humans on the same task (though different sequences of observations). The human data is [here](\"https://osf.io/rsd46/\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_episodes = 128\n",
    "sequence_length = 100\n",
    "batch_size = 64\n",
    "device = \"cpu\"\n",
    "eval_dims = [0, 1, 2]\n",
    "data = ThingsFunctionLearning(representations=np.load(\"data/backbone_reps/dinov2_vitb14_reg.npz\"))\n",
    "synthetic_data = dict(dimension=[], trial=[], correct=[], participant=[])\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    eval_losses = []\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # collect episodes first, then process in batches\n",
    "    X_eval_batch_list,Y_eval_batch_list = [], [] \n",
    "\n",
    "    for i in range(num_eval_episodes):\n",
    "        dim = eval_dims[i % len(eval_dims)] # cycle through eval_dims\n",
    "        X_episode, Y_episode = data.sample_episode(dim, sequence_length)\n",
    "        \n",
    "        prev_targets = torch.cat([torch.tensor([0]), Y_episode[:-1]])\n",
    "        target_onehot = torch.nn.functional.one_hot(prev_targets.long(), num_classes=2).float()\n",
    "        target_onehot[0] = 0.0\n",
    "\n",
    "        \n",
    "        inputs = torch.cat([target_onehot, X_episode], dim=1)\n",
    "        \n",
    "        X_eval_batch_list.append(inputs)\n",
    "        Y_eval_batch_list.append(Y_episode)\n",
    "    \n",
    "    for i in range(0, num_eval_episodes, batch_size):\n",
    "        batch_X = torch.stack(X_eval_batch_list[i:i+batch_size]).to(device)\n",
    "        batch_Y = torch.stack(Y_eval_batch_list[i:i+batch_size]).to(device)\n",
    "\n",
    "        logits_eval = model(batch_X).squeeze(-1)\n",
    "        loss_eval =  F.binary_cross_entropy_with_logits(logits_eval, batch_Y)\n",
    "        eval_losses.append(loss_eval.item())\n",
    "        \n",
    "        predictions = (torch.sigmoid(logits_eval) > 0.5).float()\n",
    "        correct_batch = (predictions == batch_Y)\n",
    "        \n",
    "        for j in range(correct_batch.shape[0]): # iterate through episodes in batch\n",
    "            episode_idx = i + j\n",
    "            dim = eval_dims[episode_idx % len(eval_dims)]\n",
    "            for k in range(correct_batch.shape[1]): # iterate through trials in episode\n",
    "                synthetic_data['dimension'].append(dim)\n",
    "                synthetic_data['trial'].append(k)\n",
    "                synthetic_data['correct'].append(correct_batch[j, k].item())\n",
    "                synthetic_data['participant'].append(episode_idx)\n",
    "\n",
    "        correct_predictions += correct_batch.sum().item()\n",
    "        total_predictions += batch_Y.numel()\n",
    "\n",
    "    avg_eval_loss = np.mean(eval_losses)\n",
    "    eval_accuracy = correct_predictions / total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = pd.DataFrame(synthetic_data)\n",
    "human_data = pd.read_csv(\"https://osf.io/rsd46/download\")\n",
    "\n",
    "synthetic_data[\"model\"] = \"Meta-Learned\"\n",
    "human_data[\"model\"] = \"Human\"\n",
    "human_data = human_data[human_data.trial <= 99] # human data has 120 choices, currently the model has 100. Therefore, we clip the last 20\n",
    "\n",
    "synthetic_data = synthetic_data[[\"dimension\", \"trial\", \"correct\", \"participant\", \"model\"]]\n",
    "human_data = human_data[[\"dimension\", \"trial\", \"correct\", \"participant\", \"model\"]]\n",
    "\n",
    "all_data = pd.concat([synthetic_data, human_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.lineplot(data=all_data, x=\"trial\", y=\"correct\", hue=\"model\", ax=ax)\n",
    "ax.set_xlabel(\"Trials\")\n",
    "ax.set_ylabel(\"p(correct)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(all_data, col=\"model\", sharey=True, row=\"dimension\")\n",
    "g.map_dataframe(sns.lineplot, x=\"trial\", y=\"correct\")\n",
    "# remove 'model = ' from the titles of subplots\n",
    "g.set_titles(col_template=\"{col_name}\", row_template=\"Dimension {row_name}\")\n",
    "# set x and y labels\n",
    "g.set_axis_labels(\"Trial\", \"p(correct)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.barplot(data=all_data, x=\"dimension\", y=\"correct\", hue=\"model\", ax=ax)\n",
    "# remove legend\n",
    "ax.legend().remove()\n",
    "fig.legend(loc='upper center', ncol=2, frameon=False, bbox_to_anchor=(0.52, 1))\n",
    "# x label\n",
    "ax.set_xlabel(\"\")\n",
    "# y label\n",
    "ax.set_ylabel(\"p(correct)\")\n",
    "# x tick labels\n",
    "tick_labels = [\"(0) Metallic/Artificial\", \"(1) Food-related\", \"(2) Animal-related\"]\n",
    "ax.set_xticklabels(tick_labels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there is a nice qualitative match in general, though individual dimensions have some discrepancies. Not sure how severe these are at the moment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/candemircan/Projects/cpi/metarep/.venv/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
