# metalign

> what happens to visual representation in a meta-learning model

## Setup

For setup, you need to do the following things:


1. Clone the repository
    ```bash
    git clone https://github.com/candemircan/metalign.git
    ```

2. Run the setup script. This requires either `curl` or `wget` to be present on your system. If you don't have either, build a virtual environment manually and install the local package.
    ```bash
    cd metalign
    bash setup.sh
    ```

3. The project currently uses the following gated repositories:

    - [imagenet-1k-wds](https://huggingface.co/datasets/timm/imagenet-1k-wds)
    - [dinov3](https://huggingface.co/facebook/dinov3-vitb16-pretrain-lvd1689m/tree/main)

    You need to log in to Hugging Face website, and accept the terms and conditions. Then you need to verify yourself via the CLI (`uv run hf auth login`).

3. The project uses a lot of external data, which is not committed to the repository. You can download the data by running the following script, which requires `wget`.
    ```bash
    bash bin/get_data.sh
    ```

4. Run any python script with
    ```bash
    uv run bin/script.py
    ```

## Folder Structure

```bash
├── bin # all the *.sh, *.slurm, *py scripts as well as any *ipynb  notebooks
├── data # all the data generated by experiments and derivative artifacts, not committed except examples
├── figures # all the standalone figures, not committed
├── metalign # the local package, contains all the custom functions and tests
├── logs # all the logs generated by the scripts
```

## To Do

### Alignment Evaluations

#### Similarity

- odd-one-out judgements on THINGS
- odd-one-out judgements on Levels (Imagenet)

#### Learning

- our category learning task

#### fMRI

- THINGS fMRI recordings
- BOLD 5000 fMRI excluding the CoCo images

#### Misc.

- Texture bias

### Down-stream evaluations

*This is taken directly from Lukas's paper.*

#### One-shot and Ten-shot classification

- Birds
- Caltech101
- Cars
- Cifar100
- Colon
- DTD
- Flowers
- ImageNet-1k
- Pets
- Places365
- US Merced

#### Distribution shift

- BREEDS benchmark with linear probes

#### Model robustness

- ImageNet-A

### Ablation Experiments

#### Ablation Over Types of Functions

- Train over functions that came from i) 1st block ii) middle block SAEs
- Train over functions that are  sampled from the neurons of the representations

#### Ablation Over Number of Functions

- Sample 20, 40, 60, 80 % of the functions and train on those. Are the results any different?
- Say we keep 50% of the functions. Sample different distributions of functions such that the average pairwise similarity between functions vary. What's the effect of different distributions on alignment and downstream?

### Considerations

- You want to do this mainly on DINO V2 CLS representations. However, also test on OpenCLIP (from which the SAE representations are extracted) as well as an ImageNet supervised ViT.
- Are we set on CoCo for training? At least use Open Images validation split for validation. Another option is to train on Open Images and then we can use CoCo brain data as well. Here, the issue is that the Open Image dataset is massive (6M), and not as popular as CoCo. You can already predict the brain responses to CoCo anyway  if you don't want to enforce all images being non overlapping between train and evals.
- Yet another option is to train on ImageNet. This would introduce overlap of images between train and eval. The images would be more representative of the types of images you get in both human and ML evals, though you definitely lose on image diversity.
- Currently, I train transformers with a hidden size of up to 128. Likely, this is not enough for ML tasks. Training has now stabilised, and you should next try 256, 512, and 768.