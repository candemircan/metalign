# metarep

> what happens to visual representation in a meta-learning model

## Setup

For setup, you need to do the following things:


1. Clone the repository
    ```bash
    git clone https://github.com/candemircan/metarep.git
    ```

2. Run the setup script. This requires either `curl` or `wget` to be present on your system. If you don't have either, build a virtual environment manually and install the local package.
    ```bash
    cd metarep
    bash setup.sh
    ```

3. The project uses a lot of external data, which is not committed to the repository. You can download the data by running the following script, which requires `wget`.
    ```bash
    bash bin/get_data.sh
    ```

4. Run any python script with
    ```bash
    uv run bin/script.py
    ```

## Folder Structure

```bash
├── bin # all the *.sh, *.slurm, *py scripts as well as any *ipynb and *qmd notebooks
├── data # all the data generated by experiments and derivative artifacts, not committed except examples
├── figures # all the standalone figures, not committed
├── metarep # the local package, contains all the custom functions and tests
├── logs # all the logs generated by the scripts, including html renders of quarto files. not committed
```

## To Do

- Should start thinking about the axes of ablations. Here are a few that come to mind:

    - **The number of training functions**: how does test performance change with the number of training functions? Some general trends are expected, and this is perhaps rather boring. Two things can be interesting here: i) we find a "breakthrough" point where the performance increases significantly, and ii) we find a point where the performance saturates, i.e. adding more training functions does not improve the performance significantly.
    - **Similarity of test functions to training functions**: how similar are the test functions to the training functions? How well does this predict the performance of the model? I expect this to be quite a straightforward positive correlation.
    - **The number and the diversity of images**: Currently, we use the same images from the THINGS dataset both for training and testing (though under different functions). Can the model also do well on held-out functions that use held-out images? Does a model simply trained on THINGS images do well on other images, such as CoCO? What about the other way around? Basically, how important is image diverstiy? Not sure at the moment how to best phrase this question.
    - **The diversity of the training functions**: this one is more interesting, and has multiple sub-questions. If we fix the average similarity between training and test functions, how does the diversity of the training functions affect the performance? Does this matter more for test functions that are less similar to the training functions? What is the average effect of inter-function training function similarity on test performance, both behaviourally and in terms of representations?