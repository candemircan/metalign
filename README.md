# metalign

> can we align visual representations to humans better through meta-learning? without using any human data? while also improving down-stream performance?

## Setup

For setup, you need to do the following things:


1. Clone the repository
    ```bash
    git clone https://github.com/candemircan/metalign.git
    ```

2. Run the setup script. This requires either `curl` or `wget` to be present on your system. If you don't have either, build a virtual environment manually and install the local package.
    ```bash
    cd metalign
    bash setup.sh
    ```

3. The project uses a lot of external data, which is not committed to the repository. You can download the data by running the following script, which requires `wget`.
    ```bash
    bash bin/get_data.sh
    ```

## Folder Structure

```bash
├── bin # all the *.sh, *.slurm, and *py scripts
├── data # all the data generated by experiments and derivative artifacts, not committed except the base config
├── figures # all the standalone figures, not committed
├── metalign # the local package, contains all the custom functions and tests
├── logs # all the logs generated by the scripts
```

## To Do

### Alignment Evaluations

#### Similarity

- [x] odd-one-out judgements on THINGS 
- [ ] odd-one-out judgements on Levels (Imagenet)

#### Learning

- [x] our category learning task
- [x] our reward learning task

#### fMRI

- [ ] THINGS fMRI recordings

#### Misc.

- [ ] Texture bias: Not sure about this because 1) I don't know if I care enough about this and 2) I would need to train imagenet heads on top of the representations, which is quite a bit of compute. A workaround might be to use the few-shot linear heads I will train anyway for down-stream evals.

### Down-stream evaluations

*This is taken directly from Lukas's paper.*

#### One-shot and Ten-shot classification

- [ ] Birds
- [ ] Caltech101
- [ ] Cars
- [ ] Cifar100
- [ ] Colon
- [ ] DTD
- [ ] Flowers
- [ ] ImageNet-1k
- [ ] Pets
- [ ] Places365
- [ ] US Merced

#### Distribution shift

- [ ] BREEDS

#### Model robustness

- [ ] ImageNet-A

### Ablation Experiments

#### Ablation Over Types of Functions

- [x] Train over functions that came from middle block SAEs
- [x] Train over functions that are sampled from the neurons of the representations
- [ ] Train a linear layer on top of the backbone directly to the SAE features. Do you really need meta-learning or is it enough to learn all the features at the same time? Actually probably makes sense to train two linear layers. one from backbone to a new feature space that is of the same size as the backbone, and then from there to the SAE features. Then we can use the intermediate feature space for comparisons. It would be comparable to the original setup, as they are both just affine transformations of the backbone features.