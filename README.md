# metalign

> can we align visual representations to humans better through meta-learning? without using any human data? while also improving down-stream performance?

## Setup

For setup, you need to do the following things:


1. Clone the repository
    ```bash
    git clone https://github.com/candemircan/metalign.git
    ```

2. Run the setup script. This requires either `curl` or `wget` to be present on your system. If you don't have either, build a virtual environment manually and install the local package.
    ```bash
    cd metalign
    bash setup.sh
    ```

3. The project uses a lot of external data, which is not committed to the repository. You can download the data by running the following script, which requires `wget` and `curl`.
    ```bash
    bash bin/get_data.sh
    ```

## Folder Structure

```bash
├── bin # all the *.sh, *.slurm, and *py scripts
├── data # all the data generated by experiments and derivative artifacts, not committed except the base config
├── figures # all the standalone figures, not committed
├── metalign # the local package, contains all the custom functions and tests
├── logs # all the logs generated by the scripts
```

## To Do

### Alignment Evaluations

#### Similarity

- [x] odd-one-out judgements on THINGS 
- [x] odd-one-out judgements on Levels (Imagenet)

#### Learning

- [x] our category learning task
- [x] our reward learning task

#### fMRI

- [x] THINGS fMRI

### How did representations change?

- [ ] t-SNE plots of the representations before and after alignment
- [ ] Class separation analysis

### Plots for the paper

- [ ] Oveview figure (good luck with that)
- [ ] Metalign vs. Baseline comparison plots for all behavioural evaluations. Have all the data, just put it together.
- [ ] Ablations vs metalign comparison plots for all behavioural evaluations. Again, have all the data.
- [ ] fMRI comparison plots for all models. Need to think about the best way to visualize this. Currently, upper row is a cortical flatmap for one model and one participant (showing $\Delta$ $R^2$), and the lower row is a bar plot showing absolute $R^2$ values for all ROIs for each participant. Both for base and metalign models. Though just using one backbone. In the Supplement should go flatmaps from other participants, other backbones, and (if we have them) ablation models.
- [ ] Representation change plots. t-SNEs and class separation analyses.

### Ablation Experiments

#### Ablation Over Types of Functions

- [x] Train over functions that came from middle block SAEs
- [x] Train over functions that are sampled from the neurons of the representations
- [x] Train a linear layer on top of the backbone directly to the SAE features. Do you really need meta-learning or is it enough to learn all the features at the same time? Actually probably makes sense to train two linear layers. one from backbone to a new feature space that is of the same size as the backbone, and then from there to the SAE features. Then we can use the intermediate feature space for comparisons. It would be comparable to the original setup, as they are both just affine transformations of the backbone features.